# Test Guide: Ollama Integration

## âœ… Integration Complete!

The Ollama local LLM integration has been successfully implemented. You can now choose between OpenAI (cloud) or Ollama (local) models for interview responses.

## ğŸ§ª How to Test

### Step 1: Ensure Ollama is Running
```bash
# Start Ollama server (if not already running)
ollama serve

# Verify gemma3:4b is installed
ollama list
```

### Step 2: Configure Settings
1. Open the web interface: http://127.0.0.1:8000
2. Click on **Settings** in the left sidebar
3. Under "LLM Provider":
   - Select **"Ollama (Local - Free)"** from the dropdown
4. Under "Ollama Model":
   - Select **"Gemma 3:4b (Recommended - 3.3GB)"**
5. Check the Ollama Status box:
   - Should show âœ… green "Ollama connected!" with installed models
   - If red/error, click "ğŸ”„ Check Ollama Status" button
6. Click **"ğŸ’¾ Save Settings"**
   - You should see: "âœ… Settings saved! Using Ollama with model gemma3:4b"

### Step 3: Test Interview
1. Click on **Interview** in the left sidebar
2. Click **"ğŸ™ï¸ Iniciar Entrevista"** to start
3. Allow microphone/screen sharing permissions
4. Type a test question in the transcript box (or speak):
   - Example: "What is Python?"
5. Press **ENTER** to send
6. Watch the AI response:
   - Should be generated by **Gemma 3:4b** (locally, no API call!)
   - Should stream in real-time
   - Check browser console (F12) for logs: "LLM Provider: ollama"

### Step 4: Verify Logs
Check the Django server console output:
```
Received transcription: What is Python?
LLM Provider: ollama
Selected model: gemma3:4b
Using LLM provider: ollama
Model: gemma3:4b
```

## ğŸ”„ Switching Back to OpenAI

If you want to switch back to OpenAI:
1. Go to **Settings**
2. Select **"OpenAI (Cloud - Paid)"** from LLM Provider dropdown
3. Select your preferred model (e.g., "GPT-4o-mini (Faster, Cheaper)")
4. Click **"ğŸ’¾ Save Settings"**

## ğŸ¯ What Was Implemented

### Backend Changes
- âœ… Added Ollama provider support in `copilot/utils.py`
- âœ… Async streaming for Ollama responses
- âœ… Dynamic provider/model selection in `copilot/consumers.py`
- âœ… Configuration settings in `interview_copilot/settings.py`

### Frontend Changes
- âœ… Settings UI with provider dropdown (OpenAI/Ollama)
- âœ… Model selection for each provider
- âœ… Ollama server status checker with visual feedback
- âœ… Settings persistence via localStorage
- âœ… Interview page uses selected provider/model

### Technical Details
- OpenAI responses: Synchronous streaming
- Ollama responses: Asynchronous streaming
- Both providers support real-time token-by-token streaming
- Provider/model settings persist across page reloads
- Settings saved in browser localStorage

## ğŸ› Troubleshooting

### Ollama Not Connected
- Make sure Ollama is running: `ollama serve`
- Check if port 11434 is accessible: http://localhost:11434/api/tags
- Verify gemma3:4b is installed: `ollama list`
- If not installed: `ollama pull gemma3:4b`

### No Response from Ollama
- Check browser console (F12) for errors
- Check Django server logs for error messages
- Verify settings were saved (refresh page and check Settings)
- Try switching to OpenAI to verify basic flow works

### Slow Responses
- Ollama performance depends on your hardware
- Gemma 3:4b is lightweight (3.3GB) but may be slower on CPU-only systems
- For better performance, ensure Ollama has GPU access
- Smaller models available: gemma3:1b (even faster but less capable)

## ğŸ“Š Performance Comparison

| Provider | Model | Speed | Cost | Quality |
|----------|-------|-------|------|---------|
| OpenAI | GPT-4o | Fast | $$$ | Excellent |
| OpenAI | GPT-4o-mini | Very Fast | $ | Very Good |
| OpenAI | GPT-3.5-turbo | Fast | $ | Good |
| Ollama | Gemma 3:4b | Medium* | FREE | Good |
| Ollama | Gemma 3:1b | Fast* | FREE | Decent |

*Speed depends on your hardware (GPU vs CPU)

## ğŸ‰ Success Criteria

You'll know it's working when:
- âœ… Settings page shows Ollama status as connected
- âœ… Interview responses come from local Gemma model
- âœ… No OpenAI API calls in server logs
- âœ… Browser console shows "LLM Provider: ollama"
- âœ… Real-time streaming works smoothly

Enjoy your free, local AI interview copilot! ğŸš€
